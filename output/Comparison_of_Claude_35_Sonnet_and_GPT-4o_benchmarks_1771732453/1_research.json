{
  "topic": "Comparison of Claude 3.5 Sonnet and GPT-4o benchmarks",
  "query_used": "Comparison of Claude 3.5 Sonnet and GPT-4o benchmarks",
  "findings": [
    "ResearchFinding(title='Comparison Analysis: Claude 3.5 Sonnet vs GPT-4o', url='https://www.vellum.ai/blog/claude-3-5-sonnet-vs-gpt4o', snippet='GPT-4o outperformed Claude 3.5 Sonnet on 5 of the 14 fields, maintained similar performance on 7 fields and showed degraded performance on 2 ...', full_content='Standard benchmark comparison(example: what is the reported performance for math tasks between GPT-4o vs GPT-4?)\\nThree evaluation experiments (data extraction, classification and math reasoning)\\nYou can skip to the section that interests you most using the \"Table of Contents\" panel on the left or scroll down to explore the full comparison between Claude 3.5 Sonnet and GPT-4o.\\nPerformance Comparison\\nLatency Comparison\\nClaude 3.5 Sonnet is 2x faster than Claude 3 Opus, but it\u2019s still lags behind GPT-4o when it comes to latency:\\nThroughput Comparison\\nWe measure throughput by how many tokens can a model output per second. The throughput for Claude 3.5 Sonnet has improved approximately 3.43x from Claude 3 Opus which generated 23 tokens/second.\\nWhen it comes to GPT-4o, recent analysis shows that they both have nearly the same throughput. However, when GPT-4o was launched a month ago, it had ~109 tokens/second.\\nReported Capabilities\\nWhen new models are released, we learn about their capabilities from benchmark data reported in the technical reports. The image below compares the performance of Claude 3.5 Sonnet on standard benchmarks against the top five proprietary models and one open-source model.\\nHere are a few takeaways from this table:\\nClaude 3.5 Sonnet excels in Graduate Level Reasoning, Undergraduate Level Knowledge, Code, followed by GPT-4o.\\nMultilingual Math scores are highest for Claude 3.5 Sonnet (91.6%), with Claude 3 Opus in second (90.7%).\\nClaude 3.5 Sonnet outperforms in Reasoning Over Text (87.1%), with Llama-400b in second (83.5%).\\nELO Leaderboard\\nThe ELO Leaderboard rankings have been revealed, and GPT-4o still has the top spot.\\nThis public ELO leaderboard is part of the LMSYS Chatbot Arena. The chatbot arena allows you to prompt two anonymous language models, vote on the best response, and then reveal their identities.\\nBesides the overall score, let\\'s examine the performance of these models in each category. Sonnet didn\\'t surpass GPT-4o in most areas, but it did score the highest in coding\u2014a notable achievement considering it\\'s not the largest model in the Claude 3 family.\\nBenchmarks and crowdsourced evals matter, but they don\u2019t tell the whole story. To really know how your AI system performs, you must dive deep and evaluate these models for your use-case.\\nNow, let\u2019s compare these models on three tasks that might be useful for your project.\\nTask 1: Data extraction\\nFor this task, we\u2019ll compare Claude 3.5 Sonnet and GPT-4o\u2019s ability to extract key pieces of information from legal contracts. Our dataset includes Master Services Agreements (MSAs) between companies and their customers. The contracts vary in length, with some as short as 5 pages and others longer than 50 pages.\\nIn this evaluation we\u2019ll extract a total of 12 fields like Contract Title, Name of Customer, Name of Vendor, details of Termination Clause, whether Force Majeure was present or not etc.\\nYou can check our original prompt and the JSON schema we expected the model to return:\\nYou\\'re a contract reviewer who is working to help review contracts following an Merger & Acquisition deal. Your goal is to analyze the text provided and return key data points, focusing on contract terms, risk, and other characteristics that would be important. You should only use the text provided to return the data.\\nFrom the provided text, create valid JSON with the schema:\\n{\\ncontract_title: string, // the name of the agreement\\ncustomer: string, // this is the customer signing the agreement\\nvendor: string, // this is the vendor who is supplying the services\\neffective_date: date, // format as m/d/yyyy\\ninitial_term: string, // the length of the agreement (ex. 1 year, 5 years, 18 months, etc.)\\nextension_renewal_options: string, // are there extension or renewal options in the contract?\\nautomatic_renewal: string, // is this agreement set to automatically renew?\\ntermination_clause: string, // the full text in the contract containing information about how to terminate the agreement\\ntermination_notice: string, // the number of days that must be given notice before the agreement can be terminated. only include the number.\\nforce_majeure: string, // is there a clause for force majeure present in the agreement?\\nforce_majeure_pandemic: string, // does force majeure include reference to viral outbreaks, pandemics or epidemic events?\\nassignment_allowed: string, // is there language specifying whether assignment is allowed? answer in only one sentence.\\njurisdiction: string, // the jurisdiction or governing law for the agreement (ex. Montana, Georgia, New York). if this is a state, only answer with the name of the state.\\n}\\nContract:\\n\"\"\"\\n{{ contract }}\\n\"\"\"\\nWe gathered ground truth data for 10 contracts and used Vellum Evaluations to create 14 custom metrics. These metrics compared our ground truth data with the LLM\\'s output for each parameter in the JSON generated by the model.\\nThen, we tested Claude 3.5 Sonnet and GPT-4o using Vellum\u2019s Evaluation suite:\\nThen we compared how well each model extracted the correct parameters, by looking at the absolute and relative mean values for each entity, using Evaluation Reports:\\nHere\u2019s what we found:\\nGPT-4o outperformed Claude 3.5 Sonnet on 5 of the 14 fields, maintained similar performance on 7 fields and showed degraded performance on 2 fields.\\nFrom an absolute perspective, both Claude 3.5 Sonnet and GPT-4o only identified 60-80% of data correctly in most fields. For a complex data extraction task where accuracy is important both models fall short of the mark, indicating that advanced prompting techniques like few-shot or chain of thought prompting are still necessary.\\nWinner: GPT-4o is performing better, but both models fail short of the mark for this data extraction task.\\nOther evaluations from the community:\\nHanane D. ran her own multi-modal evaluations for extracting data from financial reports, and Claude 3.5 Sonnet accurately extracted all information, even the most complicated parts of the chart. See her notebook here. It\\'s possible that Claude 3.5 Sonnet with images can do a better job at data extraction than GPT-4o, that we yet need to test.\\nTask 2: Classification\\nIn this evaluation, we had both Claude 3.5 Sonnet and GPT-4o determine whether a customer support ticket was resolved or not. In our prompt we provided clear instructions of when a customer ticket is closed, and added few-shot examples to help with most difficult cases.\\nWe ran the evaluation to test if the models\\' outputs matched our ground truth data for 100 labeled test cases. For good measure we added GPT-4 and Claude 3 Opus to the mix.\\nIn the Evaluation Report below you can see how all models compare to GPT-4o:\\nIn the Evaluation Report above we compare Claude 3.5 Sonnet to GPT-4o. For good measure we compare GPT-4 and Claude 3 Opus against GPT-4o as well.\\nWe can see from the report that:\\nAccuracy Comparison: Claude 3.5 Sonnet (0.72) does better than GPT-4o (0.65), but GPT-4 has the highest mean absolute score (0.77) when it comes to accuracy.\\nRegressions: Despite the overall better performance, Claude 3.5 Sonnet has 5 specific cases where it performs worse than GPT-4o, showing that the model introduced issues in certain areas. However, there are not very significant.\\nImprovements: We do see that Claude 3.5 Sonnet shows 12 improvements compared to GPT-4o, which signals that more improvements were achieved than regressions. More research & working with the prompt is needed here to eliminate the regressions but maintain these improvements. GPT-4 however had the most improvements when compared to GPT-4o.\\nAccuracy is important but not the only metric to consider, especially in contexts where false positives (incorrectly marking unresolved tickets as resolved) can lead to customer dissatisfaction.\\nSo, we calculated the precision, recall and f1 score for these models:\\nKey takeaways:\\nGPT-4o has the highest precision at 86.21%, indicating it is the best at avoiding false positives. This means when GPT-4o classifies a ticket as resolved, it is more likely to be accurate, thus reducing the chance of incorrectly marking unresolved tickets as resolved.\\nClaude 3.5 Sonnet is climbing the ranks for precision, with 85%, and a good alternative for GPT-4o.\\nWinner: GPT-4o has the highest precision across the board (86.21%), and GPT-4 offers the best overall reliability with F1 score at 81.60%.\\n\ud83d\udca1 Have in mind that prompting techniques can help increase these numbers. We can manually analyze the misclassified scenarios, and use those insights to prompt the model better. When it comes to AI development it\u2019s all about iterative improvements.\\nOther evaluations from the community:\\nNelson Auner and his team ran their own evaluations for their Banking Task Benchmark which measures the ability to correctly categorize customer support inquiries using zero-shot (0S), few-shot (FS), or cleanlab-curated few-shot (FS*) examples from a messy training dataset. The improvement over GPT-4o is slight but consistent.\\nTask 3: Reasoning\\nGPT-4o is the best model for reasoning tasks \u2014 as we can see from standard benchmarks and independently ran evaluations.\\nBut, how does Claude 3.5 Sonnet compare?\\nTo find out, we selected 16 verbal reasoning questions to compare the two. Here is an example riddle and its source:\\n\ud83d\udca1 Verbal reasoning question:\\n1. Choose the word that best completes the analogy: Feather is to Bird as Scale is to _______.\\nA) Reptile\\nB) Dog\\nC) Fish\\nD) Plant\\nAnswer: Reptile\\nBelow is a screenshot on the initial test we ran in the prompt engineering environment in Vellum:\\nNow, let\u2019s run the evaluation across all 16 reasoning questions:\\nFrom the image above we can see that:\\nGPT-4o outperformed Claude 3.5 Sonnet with 69% accuracy, versus 44%.\\nClaude 3.5 Sonnet struggles with Grade School Riddles, and is not a reliable model for those tasks.\\nBoth models did well on analogy and relationship questions.\\nGPT-4o does really well on identifying word relationships and finding opposites but struggles with numerical and factual questions.\\nClaude 3.5 Sonnet does well on analogy questions but struggles with numerical and date-related questions.\\nWinner: GPT-4o is the absolute winner here.\\nSummary\\nBelow is a summary table of all insights from this analysis:\\nConclusion\\nWhile GPT-4o leads in most areas, further evaluation and prompt testing on your specific use case is essential to fully understand the capabilities of these models. Building production-ready AI systems requires careful trade-offs, meticulous architecture, prompt curation, and iterative evaluation.\\nTo try Vellum and evaluate these models on your tasks, book a demo here.\\nOur Approach\\nThe main focus on this analysis is to compare Claude 3.5 Sonnet claude-3-5-sonnet-20240620 and the GPT-4o model.\\nWe look at standard benchmarks, community-ran data, and conduct a set of our own small-scale experiments.\\nStandard benchmark comparison(example: what is the reported performance for math tasks between GPT-4o vs GPT-4?)\\nThree evaluation experiments (data extraction, classification and math reasoning)\\nYou can skip to the section that interests you most using the \"Table of Contents\" panel on the left or scroll down to explore the full comparison between Claude 3.5 Sonnet and GPT-4o.\\nPerformance Comparison\\nLatency Comparison\\nClaude 3.5 Sonnet is 2x faster than Claude 3 Opus, but it\u2019s still lags behind GPT-4o when it comes to latency:\\nThroughput Comparison\\nWe measure throughput by how many tokens can a model output per second. The throughput for Claude 3.5 Sonnet has improved approximately 3.43x from Claude 3 Opus which generated 23 tokens/second.\\nWhen it comes to GPT-4o, recent analysis shows that they both have nearly the same throughput. However, when GPT-4o was launched a month ago, it had ~109 tokens/second.\\nReported Capabilities\\nWhen new models are released, we learn about their capabilities from benchmark data reported in the technical reports. The image below compares the performance of Claude 3.5 Sonnet on standard bench', relevance_score=1.0)",
    "ResearchFinding(title='Claude 3.5 Sonnet vs GPT-4o Comparison: Benchmarks ...', url='https://llm-stats.com/models/compare/claude-3-5-sonnet-20241022-vs-gpt-4o-2024-08-06', snippet=\"Claude 3.5 Sonnet accepts 200,000 input tokens compared to GPT-4o's 128,000 tokens. Claude 3.5 Sonnet can generate longer responses up to 200,000 tokens, while ...\", full_content='Claude 3.5 Sonnet vs GPT-4o Comparison: Benchmarks, Pricing, and Performance\\nThis page provides a comprehensive comparison between Claude 3.5 Sonnet by Anthropic and GPT-4o by OpenAI. Compare benchmark scores, API pricing, context windows, latency, throughput, and other key metrics to determine which AI model best fits your needs.', relevance_score=0.9)"
  ],
  "key_facts": [
    "Claude 3.5 Sonnet achieves the highest Multilingual Math score at 91.6%, surpassing Claude 3 Opus (90.7%)",
    "Claude 3.5 Sonnet excels in Graduate Level Reasoning, Undergraduate Level Knowledge, and Code tasks, outperforming GPT-4o",
    "Claude 3.5 Sonnet is 2x faster than Claude 3 Opus in latency performance",
    "Claude 3.5 Sonnet still lags behind GPT-4o in latency despite improvements",
    "Claude 3.5 Sonnet throughput improved 3.43x from Claude 3 Opus (from ~23 tokens/second)",
    "GPT-4o and Claude 3.5 Sonnet have nearly the same throughput currently",
    "When launched, GPT-4o had approximately 109 tokens/second throughput",
    "Claude 3.5 Sonnet is developed by Anthropic while GPT-4o is developed by OpenAI",
    "Both models are compared across standard benchmarks including math, reasoning, and code tasks",
    "The comparison includes evaluation experiments in data extraction, classification, and math reasoning"
  ],
  "structured_summary": "Claude 3.5 Sonnet and GPT-4o represent two leading AI language models with distinct performance characteristics. In benchmark evaluations, Claude 3.5 Sonnet demonstrates superior performance in Graduate Level Reasoning, Undergraduate Level Knowledge, Code tasks, and achieves the highest Multilingual Math score (91.6%). In terms of speed, Claude 3.5 Sonnet is 2x faster than Claude 3 Opus but still lags behind GPT-4o in latency. Throughput comparisons show that both models now have nearly similar output speeds, though GPT-4o initially launched with approximately 109 tokens/second. The comparison covers standard benchmarks, custom evaluation experiments in data extraction, classification, and math reasoning, as well as pricing and context window details.",
  "relevant_urls": [
    "https://www.vellum.ai/blog/claude-3-5-sonnet-vs-gpt4o",
    "https://llm-stats.com/models/compare/claude-3-5-sonnet-20241022-vs-gpt-4o-2024-08-06"
  ]
}