{
  "topic": "MiniMax M2.5 ",
  "query_used": "MiniMax M2.5 capabilities performance benchmarks pricing real-world testing user experiences",
  "findings": [
    "ResearchFinding(title='MiniMax M2.5: Built for Real-World Productivity.', url='https://www.minimax.io/news/minimax-m25', snippet='It costs just $1 to run the model continuously for an hour at a rate of 100 tokens per second. At 50 tokens per second, the cost drops to $0.30.', full_content=\"Today we're introducing our latest model, MiniMax-M2.5.\\nExtensively trained with reinforcement learning in hundreds of thousands of complex real-world environments, M2.5 is SOTA in coding, agentic tool use and search, office work, and a range of other economically valuable tasks, boasting scores of 80.2% in SWE-Bench Verified, 51.3% in Multi-SWE-Bench, and 76.3% in BrowseComp (with context management).\\nTrained to reason efficiently and decompose tasks optimally, M2.5 exhibits tremendous speed in performing complicated agentic tasks, completing the SWE-Bench Verified evaluation 37% faster than M2.1, matching the speed of Claude Opus 4.6.\\nM2.5 is the first frontier model where users do not need to worry about cost, delivering on the promise of intelligence too cheap to meter. It costs just $1 to run the model continuously for an hour at a rate of 100 tokens per second. At 50 tokens per second, the cost drops to $0.30. We hope that the speed and cost effectiveness of M2.5 enable innovative new agentic applications.\\nCoding\\nIn programming evaluations, MiniMax-M2.5 saw substantial improvements compared to previous generations, reaching SOTA levels. The performance of M2.5 in multilingual coding tasks is especially pronounced.\\nA significant improvement from previous generations is M2.5's ability to think and plan like an architect. The Spec-writing tendency of the model emerged during training: before writing any code, M2.5 actively decomposes and plans the features, structure, and UI design of the project from the perspective of an experienced software architect.\\nM2.5 was trained on over 10 languages (including Go, C, C++, TypeScript, Rust, Kotlin, Python, Java, JavaScript, PHP, Lua, Dart, and Ruby) across more than 200,000 real-world environments. Going far beyond bug-fixing, M2.5 delivers reliable performance across the entire development lifecycle of complex systems: from 0-to-1 system design and environment setup, to 1-to-10 system development, to 10-to-90 feature iteration, and finally 90-to-100 comprehensive code review and system testing. It covers full-stack projects spanning multiple platforms including Web, Android, iOS, and Windows, encompassing server-side APIs, business logic, databases, and more, not just frontend webpage demos.\\nTo evaluate these capabilities, we also upgraded the VIBE benchmark to a more complex and challenging Pro version, significantly increasing task complexity, domain coverage, and evaluation accuracy. Overall, M2.5 performs on par with Opus 4.5.\\nWe focused on the model's ability to generalize across out-of-distribution harnesses. We tested performance on the SWE-Bench Verified evaluation set using different coding agent harnesses.\\n- On Droid: 79.7(M2.5) > 78.9(Opus 4.6)\\n- On OpenCode: 76.1(M2.5) > 75.9(Opus 4.6)\\nSearch and Tool calling\\nEffective tool calling and search are prerequisites for a model's ability to autonomously handle more complex tasks. In evaluations on benchmarks such as BrowseComp and Wide Search, M2.5 achieved industry-leading performance. At the same time, the model's generalization has also improved \u2014 M2.5 demonstrates more stable performance when facing unfamiliar scaffolding environments.\\nIn research tasks performed by professional human experts, using a search engine is only a small part of the process; most of the work involves deep exploration across information-dense webpages. To address this, we built RISE (Realistic Interactive Search Evaluation) to measure a model's search capabilities on real-world professional tasks. The results show that M2.5 excels at expert-level search tasks in real-world settings.\\nCompared to its predecessors, M2.5 also demonstrates much better decision-making when handling agentic tasks: it has learned to solve problems with more precise search rounds and better token efficiency. For example, across multiple agentic tasks including BrowseComp, Wide Search, and RISE, M2.5 achieved better results with fewer rounds, using approximately 20% fewer rounds compared to M2.1. This indicates that the model is no longer just getting the answer right, but is also reasoning towards results in more efficient paths.\\nOffice work\\nM2.5 was trained to produce truly deliverable outputs in office scenarios. To this end, we engaged in thorough collaboration with senior professionals in fields such as finance, law, and social sciences. They designed requirements, provided feedback, participated in defining standards, and directly contributed to data construction, bringing the tacit knowledge of their industries into the model's training pipeline. Based on this foundation, M2.5 has achieved significant capability improvements in high-value workspace scenarios such as Word, PowerPoint, and Excel financial modeling. On the evaluation side, we built an internal Cowork Agent evaluation framework (GDPval-MM) that assesses both the quality of the deliverable and the professionalism of the agent's trajectory through pairwise comparisons, while also monitoring token costs across the entire workflow to estimate the model's real-world productivity gains. In comparisons against other mainstream models, it achieved an average win rate of 59.0%.\\nEfficiency\\nBecause the real world is full of deadlines and time constraints, task completion speed is a practical necessity. The time it takes a model to complete a task depends on its task decomposition effectiveness, token efficiency, and inference speed. M2.5 is served natively at a rate of 100 tokens per second, which is nearly twice that of other frontier models. Further, our reinforcement learning setup incentivizes the model to reason efficiently and break down tasks optimally. Due to these three factors, M2.5 delivers a significant time savings in complex task completion.\\nFor example, when running SWE-Bench Verified, M2.5 consumed an average of 3.52 million tokens per task. In comparison, M2.1 consumed 3.72M tokens. Meanwhile, thanks to improvements in capabilities such as parallel tool calling, the end-to-end runtime decreased from an average of 31.3 minutes to 22.8 minutes, representing a 37% speed improvement. This runtime is on par with Claude Opus 4.6's 22.9 minutes, while the total cost per task is only 10% that of Claude Opus 4.6.\\nFor example, when running SWE-Bench Verified, M2.5 consumed an average of 3.52 million tokens per task. In comparison, M2.1 consumed 3.72M tokens. Meanwhile, thanks to improvements in capabilities such as parallel tool calling, the end-to-end runtime decreased from an average of 31.3 minutes to 22.8 minutes, representing a 37% speed improvement. This runtime is on par with Claude Opus 4.6's 22.9 minutes, while the total cost per task is only 10% that of Claude Opus 4.6.\\nCost\\nOur goal in designing the M2-series of foundation models is to power complex agents without having to worry about cost. We believe that M2.5 is close to realizing this goal. We\u2019re releasing two versions of the model, M2.5 and M2.5-Lightning, that are identical in capability but differ in speed. M2.5-Lightning has a steady throughput of 100 tokens per second, which is two times faster than other frontier models, and costs $0.3 per million input tokens and $2.4 per million output tokens. M2.5, which has a throughput of 50 tokens per second, costs half that. Both model versions support caching. Based on output price, the cost of M2.5 is one-tenth to one-twentieth that of Opus, Gemini 3 Pro, and GPT-5.\\nAt a rate of 100 output tokens per second, running M2.5 continuously for an hour costs $1. At a rate of 50 TPS, the price drops to $0.3. To put that into perspective, you can have four M2.5 instances running continuously for an entire year for $10,000. We believe that M2.5 provides virtually limitless possibilities for the development and operation of agents in the economy. For the M2-series, the only problem that remains is how to continually push the frontier of model capability.\\nAt a rate of 100 output tokens per second, running M2.5 continuously for an hour costs $1. At a rate of 50 TPS, the price drops to $0.3. To put that into perspective, you can have four M2.5 instances running continuously for an entire year for $10,000. We believe that M2.5 provides virtually limitless possibilities for the development and operation of agents in the economy. For the M2-series, the only problem that remains is how to continually push the frontier of model capability.\\nImprovement Rate\\nOver the three and a half months from late October to now, we have successively released M2, M2.1, and M2.5, with the pace of model improvement exceeding our original expectations. For instance, in the highly-regarded SWE-Bench Verified benchmark, the rate of progress of the M2-series has been significantly faster than that of peers such as the Claude, GPT, and Gemini model families.\\nRL Scaling\\nOne of the key drivers of the aforementioned developments is the scaling of reinforcement learning. As we train our models, we also benefit from their abilities. Most of the tasks and workspaces that we perform in our company have been made into training environments for RL. To date, there are already hundreds of thousands of such environments. At the same time, we did plenty of work on our agentic RL framework, algorithms, reward signals, and infrastructure engineering to support the continued scaling of our RL training.\\nForge \u2013\u2013 Agent-Native RL Framework\\nWe designed an agent-native RL framework in-house, called Forge, which introduces an intermediary layer that fully decouples the underlying training-inference engine from the agent, supporting the integration of arbitrary agents and enabling us to optimize the model's generalization across agent scaffolds and tools. To improve system throughput, we optimized asynchronous scheduling strategies to balance system throughput against sample off-policyness, and designed a tree-structured merging strategy for training samples, achieving approximately 40x training speedup.\\nAgentic RL Algorithm and Reward Design\\nOn the algorithm side, we continued using the CISPO algorithm we proposed at the beginning of last year to ensure the stability of MoE models during large-scale training. To address the credit assignment challenge posed by long contexts in agent rollouts, we introduced a process reward mechanism for end-to-end monitoring of generation quality. Furthermore, to deeply align with user experience, we evaluated task completion time through agent trajectories, achieving an optimal trade-off between model intelligence and response speed.\\nWe will release a more comprehensive introduction to RL scaling soon in a separate technical blogpost.\\nMiniMax Agent: M2.5 as a Professional Employee\\nM2.5 has been fully deployed in MiniMax Agent, delivering the best agentic experience.\\nWe have distilled core information-processing capabilities into standardized Office Skills deeply integrated within MiniMax Agent. In MAX mode, when handling tasks such as Word formatting, PowerPoint editing, and Excel calculations, MiniMax Agent automatically loads the corresponding Office Skills based on file type, improving the quality of task outputs.\\nFurthermore, users can combine Office Skills with domain-specific industry expertise to create reusable Experts tailored to specific task scenarios.\\nTake industry research as an example: by merging a mature research framework SOP (standard operating procedure) with Word Skills, the Agent can strictly follow the established framework to automatically fetch data, organize analytical logic, and output properly formatted research reports \u2014 rather than merely generating a raw block of text. In financial modeling scenarios, by combining an organization's proprietary modeling standards with Excel Skills, the Agent can follow specific risk control logic and calculation standards to automatically generate and validate complex financial models, rather than simply outputting a basic spreadsheet.\\nTo date, users have built over 10,000 Experts on MiniMax Agent, and t\", relevance_score=1.0)",
    "ResearchFinding(title='Minimax M2.5 is not worth the hype compared to Kimi 2.5 ...', url='https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/', snippet=\"It's a cheap model, works fine for what it is. It benchmark well with agent tests. It is not at the same level as big models. It doesn't store ...\", full_content=\"Minimax M2.5 is not worth the hype compared to Kimi 2.5 and GLM 5\\nI used opencode with exa; to test the latest GLM 5, Kimi 2.5 and Minimax M2.5, along with Codex 5.3 and Opus 4.6 (in its own cli) to understand how would they work on my prompt. And the results were very disappointing.\\nDespite all these posts, videos and benchmarks stating how awesome minimax m2.5 is, it failed my test horribly given the same environment and prompt, that the others easily passed.\\nMinimax kept hallucinating various solutions and situations that didn't make any sense. It didn't properly search online or utilized the available documentation properly. So, I wonder how all those benchmarks claiming minimax as some opus alternative actually made their benchmark.\\nI saw a few other real benchmarks where Minimax M2.5 actually was way below Haiku 4.5 while GLM 5 and Kimi went above Sonnet 4.5; personally it felt like that as well. So at the increased price points from all these providers, its very interesting. Though neither are on opus or codex level.\\nI did not test the same prompt with gemini, or couldn't test it, to be more precise due to circumstances. But I have a feeling Gemini 3 Pro would be similar to Kimi and GLM 5, maybe just a bit higher.\\nWhat is your experience with Minimax compared to GLM and Kimi?\", relevance_score=1.0)",
    "ResearchFinding(title='MiniMax M2.5 - SOTA in Coding and Agent, Designed for ...', url='https://www.minimax.io/models/text', snippet='Designed for high-throughput, low-latency production environments. M2.5 delivers industry-leading coding and reasoning capabilities at a fraction of the cost.', full_content='MiniMax M2.5\\nSOTA in Coding and Agent, designed for Agent Universe\\nDesigned for high-throughput, low-latency production environments. M2.5 delivers industry-leading coding and reasoning capabilities at a fraction of the cost.\\nPerformance Benchmark\\nCompared to its predecessor, M2.5 demonstrates greater decision-making maturity in handling Agentic tasks: it has learned to solve problems with more precise search iterations and better token efficiency.\\nM2.5 has achieved significant capability improvements in advanced workspace scenarios such as Word, PPT, Excel financial modeling, and more.\\nBy combining reinforcement learning-optimized task decomposition with thinking token efficiency, M2.5 delivers significant advantages in both speed and cost when completing complex tasks.\\nM2.5 is available in 100 TPS and 50 TPS versions, with output pricing at just 1/10 to 1/20 of comparable models.\\nCoding Core Benchmark Scores Open-Source SOTA\\nM2.5 has reached the level of tier-one industry models. On the multilingual task benchmark Multi-SWE-Bench, M2.5 achieved the best performance in the industry.\\nM2.5 Showcases\\nSee What M2.5 Can Do\\nAll examples below were generated by MiniMax M2.5 in a single shot. Click cards to enlarge.\\nInvite friends, earn benefits\\nSubscribe to a Coding Plan got a 10% discount, while the inviter got a 10% rebate!\\nInvite friends, earn benefits\\nSubscribe to a Coding Plan got a 10% discount, while the inviter got a 10% rebate!\\n01 / Access Method\\nQuick API Integration\\nTwo API versions: M2.5 and M2.5-highspeed with identical results but faster speed. Full automatic Cache support, no configuration needed.\\n02 / Access Method\\nFor AI Coding Tools\\nModel weights have been fully open-sourced on HuggingFace. It is recommended to use vLLM or SGLang for deployment to achieve optimal performance.\\nSubscribe to the Coding Plan\\nThe price remains unchanged, while performance has significantly improved. Coding Plan users now automatically benefit from higher inference speeds.\\nRead MoreMiniMax Agent Integration\\nThe general Agent platform based on M2.5 is now fully open. Experience the best programming assistance and logical reasoning capabilities without any development required.\\nRead MoreOpen Source and Local Deployment\\nWe are committed to giving back to the community. M2.5 has been synchronously open-sourced on HuggingFace and GitHub, supporting private cluster deployment and fine-tuning.\\nRead More03 / Access Method\\nLocal Private Deployment\\nModel weights are fully open-sourced on HuggingFace. We recommend SGLang or vLLM for optimal performance, with Transformers and Ktransformers also supported.', relevance_score=0.9)"
  ],
  "key_facts": [
    "MiniMax M2.5 is the latest model from MiniMax, extensively trained with reinforcement learning in hundreds of thousands of complex real-world environments",
    "Achieves SOTA performance in coding, agentic tool use and search, office work, with scores of 80.2% in SWE-Bench Verified, 51.3% in Multi-SWE-Bench, and 76.3% in BrowseComp",
    "Completes SWE-Bench Verified evaluation 37% faster than M2.1, matching the speed of Claude Opus 4.6",
    "Priced at $1/hour at 100 tokens per second or $0.30/hour at 50 tokens per second, representing 1/10 to 1/20 the cost of comparable models",
    "Features spec-writing tendency that emerged during training - actively decomposes and plans features, structure before writing code",
    "Demonstrates greater decision-making maturity in handling agentic tasks with more precise search iterations and better token efficiency",
    "Shows significant capability improvements in advanced workspace scenarios including Word, PPT, and Excel financial modeling",
    "Available in two versions: 100 TPS and 50 TPS, designed for high-throughput, low-latency production environments",
    "Achieved best performance in the industry on multilingual task benchmark Multi-SWE-Bench, reaching tier-one industry model levels",
    "Users report mixed real-world experiences, with some finding it disappointing compared to competitors like Kimi 2.5 and GLM 5",
    "Some users report issues with hallucination and poor online search/documentation utilization in practical tests",
    "Benchmarks show mixed results, with some placing it below Haiku 4.5 while competitors like GLM 5 and Kimi performed above Sonnet 4.5",
    "Can think and plan like an architect, showing substantial improvements in multilingual coding tasks",
    "Combines reinforcement learning-optimized task decomposition with thinking token efficiency for significant speed and cost advantages",
    "First frontier model positioned as delivering 'intelligence too cheap to meter'"
  ],
  "structured_summary": "MiniMax M2.5 is positioned as a state-of-the-art AI model designed for real-world productivity, particularly excelling in coding, agentic tasks, and office work. The model demonstrates impressive benchmark performance with 80.2% on SWE-Bench Verified and industry-leading scores on other evaluations. It offers significant speed improvements over its predecessor (37% faster) while maintaining competitive performance with models like Claude Opus 4.6. A key differentiator is its cost-effectiveness, priced at $1/hour for continuous operation at 100 tokens per second, representing a fraction of comparable models' costs. The model features advanced planning capabilities with a spec-writing tendency that emerged during training, allowing it to decompose and plan features before implementation. However, user experiences appear mixed, with some real-world tests showing disappointing results compared to competitors like Kimi 2.5 and GLM 5, including issues with hallucination and poor documentation utilization. The model is available in two throughput versions and is specifically designed for high-throughput, low-latency production environments.",
  "relevant_urls": [
    "https://www.minimax.io/news/minimax-m25",
    "https://www.reddit.com/r/opencodeCLI/comments/1r5vv6g/minimax_m25_is_not_worth_the_hype_compared_to/",
    "https://www.minimax.io/models/text"
  ]
}